#### 1. 输入、输出
- 输入：Bert模型通过查询字向量表将文本中的每个字转换为一维向量，作为模型输入，同时还包括另外两个部分
	- 本文向量：该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字/ 词的语义信息相融合
	- 位置向量：由于出现在文本不同位置的字/词所携带的语义信息存在差异，BERT对不同位置的字/词分别附加一个不同的向量以作区分
- 输出：输入各字对应的融合全文语义信息后的向量表示
#### 2. 模型的预训练任务
Bert有两个预训练任务：Masked LM和Next Sentence Prediction
###### 2.1 Masked LM
- 任务描述：给定一句话，随机抹去这句话中的一个或几个词，要求根据剩余词汇预测被抹去的词分别是什么。这和我们高中英语的完型填空类似。
- Mask策略：在一句话中随机选择15%的词汇用于预测。对于在原句中被抹去的词汇，80%的情况用特殊符号\[MASK\]替换，10%的情况采用一个任意词替换，剩余10%情况下保持原词汇不变。
	- 原因
		- 在后续的微调任务中语句中不会出现\[MASK\]标记
		- 预测一个词汇时，模型并不知道输入对应位置的词汇时否为正确的词汇（10%概率），这迫使模型更多的依赖上下文信息去预测词汇，并且赋予了模型一定的纠错能力。
###### 2.2 NextSentence Prediction
- 任务描述：给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后。
- 预训练策略：从文本语料库中随机选择50%错误语句对进行训练，与Masked LM任务相结合，使得模型能够更准确的刻画语句乃至篇章层面的语义信息。
- BERT通过对Masked ML和 NextSentence Prediction任务进行联合训练，使模型输出的每个字/词的向量表示都能尽可能全面准确的刻画输入文本（单句或语句对）的整体信息，为后续的微调任务提供更好的模型参数初始值。
#### BERT模型结构
使用Transformer Encoder组装
| Model | Transformer Layer | hidden size | num head | parameters|
| - | - | - | - | - | 
|  BERT-Large | 24 | 1024 | 16 | 340M |
| BERT-Base | 12 | 768 | 12 | 110M |
| BERT-Base, Chinese | 12 | 768 |12 | 110M |





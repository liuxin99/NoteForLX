## Confusionset-guided Pointer Networks for Chinese Spelling Check
作者：Dingmin Wang etc.

期刊：ACL2019

### Abstract
本文提出了一种用于汉语拼写检查的模糊集引导的指针网络。更具体地说，我们的方法利用现成的混淆集来指导汉字生成。为此，我们的新Seq2Seq模型通过指针网络共同学习从输入的句子中复制正确的字符，或者从混淆集而不是从整个词汇表中生成字符。我们在三个有人工标注的数据集上进行了实验，结果表明，我们提出的生成模型在F1分数上大大超过所有竞争对手的模型，最高可达20%，在三个数据集上实现了最先进的性能。

### 1. Introduction
在我们的日常写作中，存在着不同类型的错误，其中经常发生的是由于字符在发音、形状和/或意义上的相似性而拼写错误。拼写检查是一项检测和纠正这种有问题的语言使用的任务。尽管这些工具很有用，但检测和修复自然语言(尤其是中文)中的错误仍远未解决。值得注意的是，汉语与其他按字母顺序排列的语言(如英语)有很大不同。首先，中文单词之间没有分隔符。其次，错误检测任务是困难的，因为它的上下文敏感的性质，即错误往往只能确定在短语/句子级别，而不是字符级别。
本文提出了一种新的汉语拼写检查(CSC)任务的神经网络结构。对于当前的任务，可以直观地看到，生成的句子和输入的句子通常具有大部分相同的字符，句子结构也相同，只有少数字符不正确。这与其他生成任务(例如神经机器翻译或对话框翻译)不同，在这些任务中，输出与输入会有很大的不同。
为此，本文提出了一种新的基于混淆集的复制机制，该机制比竞争对手的方法获得了显著的性能提升。 Copy mechanisms(Gulcehre et al.， 2016)，使通过指向直接从输入中复制单词成为可能，为CSC任务提供了一个极其合适的归纳偏差。 更具体地说，当出现错误字符时，我们的模型共同学习选择适当的字符来复制或从词汇表中生成正确的字符。 然而，我们的工作的明显的新奇之处是将confusisets与指针网络(Pointer Networks)融合在一起，这有助于减少搜索空间并极大地提高生成正确字符的概率。 在三个基准数据集上的实验结果表明，我们的模型优于所有竞争对手的模型，获得了高达20%的性能增益。

### 2. Our Proposed Model
给定一个输入，我们将输入语句表示为$X = {c^s_1, c^s_2，···，c^s_n}$，其中$c_i$为中文字符,$n$为字符个数。我们将X映射为一个输出句子$Y = {c^t_1, c^t_2，···，c^t_n}$，即最大化概率P (Y |X)。我们的模型由一个类似于(Sutskever et al.， 2014)的编码器和解码器组成，如图1所示。编码器将X通过双向BiLSTM映射到高维表示。解码器也是一个具有注意机制(Bahdanau等，2014)的循环神经网络，用于注意编码表示，并每次生成一个字符Y。在我们的设置中，Y的长度被限制为等于X的长度。
![[attachments/Pasted image 20220712160318.png]]
**Confusionset M**
混淆集是一个准备好的字符集，它由常见的混淆字符组成，在拼写错误检测和纠正中起着关键作用。大多数汉字在字形和读音上都有相似之处。根据网上收集的错误字统计结果(Liu et al.， 2010)，其中83%的错误字与语音相似度有关，48%的错误字与相关汉字之间的视觉相似度有关。为了减少搜索空间，同时确保不排除目标字符，我们构建了一个混淆集矩阵$M \in R^{n*w}$，其中w是词汇表的大小，n对应X中的字符数，其中每个元素为0或1。以输入“这使我永生难望”为例，第7个字符“望” 是拼写错误，混淆集为”汪圣忘晚往完万网···”。在M\[7\]中，这些混淆词出现的位置将被设置为1，剩下的设置为0。

#### 2.1 Encoder
在深入模型之前，我们首先给出一个字符层面的推断。考虑到汉字的特点，汉字和一些基于字母的语言(如英语)不一样，单词之间没有明确的分隔符，因此我们的神经网络模型是在字符层面运行的。其中一个原因是即使是最先进的分词器，也会存在一些分词错误，而出现拼写错误的文本会加剧这一现象。不正确的分割结果可能会影响编码器在X中捕获语义表示。
编码器读取X并输出一个向量序列，与句子中的每个单词相关联，在解码过程中通过软注意机制有选择地访问这些向量。我们使用一个双向LSTM网络来获得每个时间步i的隐藏状态$h^s_i$:
$$h^s_i = BiLSTM(h^s_{i−1}, e^s_i)$$
其中$h^s_i$为前向隐藏状态$\overleftarrow{h^s_i}$和后向隐藏状态$\overrightarrow{h^s_i}$的拼接，$e^s_i$为$c^s_i$在X中的字符嵌入。

#### 2.1 Decoder
解码器利用另一个LSTM产生下一个目标字符的分布，给定源向量$[h^s_1, h^s_2,···,h^s_n]$，之前生成的目标字符$\hat Y_{\lt j} = [\hat c^t_1, \hat c^t_1，···，\hat c^t_1]$，以及$M \in R^{n*w}$,
$$h^t_j = LSTM(h^t_{j-1},e^t_{j-1})$$
式中$h^t_j$为目标句至第j个词的摘要，$e^t_j$为$c^t_{j-1}$的词嵌入。注意，在训练期间，标准答案$c^t_{j-1}$被输入网络来预测$c^t_j$，而在测试时，使用最可能的$c^t_{j-1}$。
我们用一个基于注意力的模型扩展了这个解码器(Bahdanau等人，2014;Luong et al.， 2015)，其中，在每个时间步t，使用注意力机制(Vinyals et al.， 2015)，为编码器的每个隐藏状态$h^s_i$计算一个注意力分数$a^s_i$。在数学上，
$$u_i = v^T tanh(W_1 h^t_j + W^2 h^s_i )$$
$$a_i = softmax(u_i)
$$
$$h^{t'}_j= \sum^n_{i=0} a^i h^s_i$$
源向量与各自的注意力权值相乘，并加到一个新的向量，作为源向量的汇总，$h^{t'}_j$。然后$h^{t'}_j$与当前解码器隐藏状态$h^t_j$交互产生上下文向量$C_j$:
$$C_j = tanh(W (h^t_j ; h^{t'}_j  )$$
其中$U、W_1、W_2、W$为模型可训练参数。然后$C_j$被用于生成两个分布:一个是在词汇表上，通过对$C_j$应用仿射变换，然后是softmax，
$$P_{vocab} = softmax(W_{vocab} C_j)$$
另一个是在输入句子上使用复制机制。此外，我们在X中添加了对应字符$c^s_j$的位置信息，$Loc_j$，这允许解码器在每个时间步中拥有先前(软)对齐的知识。$Loc_j$是一个长度为n的向量，初始化为0，在时间步长j时，$Loc_j$的第j个元素设为1，其他元素设为0。在输入句子上生成分布的隐藏状态如下:
$$L_j = softmax(W_i[W_g C_j; Loc_j])$$
其中“ ; ”表示连接操作。为了训练指针网络，我们定义解码时间步长 j 处的位置标签为:
$$
L^{loc}_j= 
\begin{cases}
max(z), & if  \exists z \text{   } s.t. c^t_j = X[z] \\
n + 1, & otherwise
\end{cases}
$$
位置n+1是一个故意连接到X末尾的标记，它允许我们计算损失函数，即使输入语句中不存在$c^t_j$。则定义$L^t$与$L^{loc}_t$之间的损失为:
$$Loss_l =  \sum^m_i − log L_j[L^{loc}_j ]$$
在解码时，$\hat c^t_j$定义为:
$$
$\hat c^t_j$= 
\begin{cases}
arg max(L_j), &if arg max(L_j) != n + 1 \\
arg max(P_{vocab} \odot M [j]), & otherwise
\end{cases}
$$
其中$\odot$为逐元素乘法，使用$M [j]$来限制生成单词的范围，假设正确的字符包含在错误字符对应的混淆集中。

### Experiments
**Train data**
我们使用(Wang et al.， 2018)中提出的一种自动方法处理后的包含拼写错误(视觉上或语音上类似的字符)的大型标注数据集。此外，三种人工标注训练数据集中的一小部分(Wu等人，2013;Yu等人，2014;Tseng et al.， 2015)也包含在我们的训练数据中。

**Test data**
为了评估我们提出的模型的有效性，我们在CSC的三个共享任务的基准数据集上测试我们的训练模型(Wu等人，2013;Yu等人，2014;Tseng等人，2015)。由于这些测试数据集是用繁体字编写的，我们使用OpenCC5将其转换为简体字。
实验数据统计信息的详细信息，包括我们模型中使用的训练数据集、测试数据集和Confusionsets，如表1所示。
![[attachments/Pasted image 20220712193105.png]]
**Evaluation metrics**
我们采用精确率、召回率和F1分数作为我们的评价指标，这些指标在CSC任务中被广泛应用。

**Baseline models**
我们将我们的模型与CSC的两种基线方法进行了比较:一种是带有预先构造的混淆集(LMC)的n元语言建模，由于其简单和强大，它在CSC中得到了广泛的应用(Liu et al., 2013; Yu and Li, 2014; Xie et al., 2015). 利用混淆集替换句子中的字符，计算替换前后的句子概率，然后根据概率判断句子是否存在拼写错误。我们重新实现了(Xie et al.， 2015)中提出的pipeline;另一种是序列标记法(sequence labeling method, SL)，该方法将中文拼写错误检测转换为对字符的序列标记问题，其中正确和错误的字符分别被标记为1和0。我们遵循基线模型(Wang et al.， 2018)，该模型实现了基于LSTM的序列标记模型。

**Model Hyperparameters**
根据验证集的结果选择训练超参数。在编码器和解码器中，word embedding的维数设置为300，隐藏向量设置为512。注意向量的维数也设置为512,dropout rate设置为0.5进行正则化。采用小批量Adam (Kingma and Ba, 2014)算法对目标函数进行优化。批大小和基本学习率设置为64和0.001。

**Results**
如表2所示，我们将混淆集引导的指针网络与两种基线方法进行比较。意料之中的是，除了两个precision结果低于LMC之外，我们的模型在detection-level和correction-level评估方面都比其他模型不断提高性能。原因之一可能是与SL相比，将拼写检查作为字符级别的分类任务，当前timepstep的可用信息有一定的限制，而我们的生成模型可以通过注意机制同时利用位置信息和整个输入信息，复制机制也使解码更加有效。对于LMC，如何设置一个阈值概率来判断给定的句子是否正确还有待探索，正如(Jia et al.， 2013)所报道的，在精确度和召回率之间存在很大的权衡。
![[attachments/Pasted image 20220712193425.png]]

**Utility of M**
具体来说，通过比较我们的−和我们的+的实验结果，我们可以观察到后者取得了更好的性能，这验证了利用混淆集的有效性，有助于提高生成正确目标字符的概率。

### 4. Discussion and Future Work
在我们的日常汉语写作中，存在着各种各样的语言使用问题，本文提到的拼写错误就是其中之一。这种拼写错误主要是由于汉字在音、形、意等方面的相似性而产生的，其任务是检测出拼写错误的单词，然后将其替换为相应的正确单词。除了上面提到的拼写错误，语法错误在我们的中文写作中也很常见，这就要求我们通过插入、删除甚至重新排序来纠正错误的句子。例如“我真<font color="red">不</font>不明白，为啥他要自杀”，为了保证句子的正确性，需要删除红色的字符。然而，我们的模型无法处理这样的错误，因为我们限制了生成句子的长度，使其与输入句子的长度相同，以便将Confusionsets合并到我们的模型中，作为一个指导资源。
在未来的工作中，我们希望将本文提出的这一思路扩展到通过生成模型来训练一个能够处理不同类型错误的模型，因为生成模型可以生成不同长度的结果。一个问题是我们需要重新考虑如何将confusisets合并到编码器-解码器架构中。

### 5. Related Work
大多数CSC相关研究是一系列共享任务的结果(Wu et al.， 2013;Yu等人，2014;Tseng等人，2015;Fung等人，2017;高琦等，2018)，涉及对给定句子的拼写错误进行自动检测和纠正。CSC的早期工作主要集中在非监督方法，如带有预构造混淆集的语言模型(Liu等人，2013;Yu和Li, 2014)。随后，一些工作将CSC作为一个顺序标记问题，其中条件随机场(CRF) (Lafferty等人，2001)，门控递归网络(Hochreiter和Schmidhuber, 1997;Chung et al.， 2014)被用来建模问题(Zheng et al.， 2016;谢等人，2017;Wu et al.， 2018)。最近，基于神经网络的序列到序列学习(Seq2Seq)在各种自然语言处理(NLP)任务中取得了一系列显著的成功(Sutskever等人，2014;Cho et al.， 2014)，生成模型也被应用于拼写检查任务，将其视为一个编码器-解码器(Xie et al.， 2016;葛等人，2018)。

### 6. Conclusion and Future Work
针对中文拼写检查(CSC)任务，提出了一种新颖的端到端混淆集引导编码器-解码器模型。通过引入带有复制机制的Confusionsets，我们提出的方法在竞争基线上获得了巨大的性能增益，证明了它在CSC任务上的有效性。
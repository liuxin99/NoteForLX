### SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check
作者： Xingyi Cheng etc. （阿里）
会议： 2020ACL

### Abstract
汉语拼写检查是一项检测和纠正汉语自然语言拼写错误的任务。已有的方法尝试将汉字之间的相似性知识纳入其中。但是，它们要么将相似性知识作为外部输入资源，要么将其作为启发式规则。本文提出通过一种专门的图卷积网络(SpellGCN)，将语音和视觉相似度知识整合到语言模型中。该模型在字符上构建一个图，SpellGCN学习将这个图映射到一组相互依赖的字符分类器。这些分类器应用于另一个网络(如BERT)提取的表示，使整个网络端到端可训练。实验在三个人工标注的数据集上进行。与以前的模型相比，我们的方法取得了很大的优势。
### 1. Introduction
拼写错误在我们的日常生活中很常见，通常是由人类书写、自动语音识别和光学字符识别系统引起的。在这些错误中，由于字符之间的相似性，经常会出现拼写错误。在汉语中，许多汉字在音韵和视觉上是相似的，但在语义上却非常不同。根据Liu et al.(2010)，大约83%的错误与语音相似有关，48%的错误与视觉相似有关。中文拼写检查(CSC)任务的目的是发现和纠正这种滥用中文的情况。CSC仍然是一个具有挑战性的任务。值得注意的是，由于中文的语言性质，中文的拼写检查与英语非常不同。汉语是一种由许多象形文字组成的语言，没有分隔符。每个角色的意思也会随着语境的变化而发生巨大的变化。因此，一个CSC系统需要识别语义并聚合周围的信息以进行必要的修改。
以前的方法遵循生成模型的路线。他们使用了两种语言模型(Liu et al.， 2013, 2010;Yu和Li, 2014)或Seq2Seq模型(Wang et al.， 2019)。为了融合字符之间相似性的外部知识，它们中的一些利用了包含一组相似字符对的混淆集。例如，Yu和Li(2014)提出通过检索混淆集产生多个候选词，然后通过语言模型进行过滤。Wang等人(2019)使用指针网络从混淆集复制相似的字符。这些方法试图利用相似度信息来限制候选者，而不是明确地建模字符之间的关系。
在本文中，我们提出了一种新的拼写检查卷积图网络(SpellGCN)，它捕获语音/形状的相似性，并探索字符之间的先验依赖关系。具体地说，构造了两个对应的发音和形状关系相似图。SpellGCN将图形作为输入，并在相似字符之间的交互后为每个字符生成一个向量表示。然后，这些表示被构造成一个字符分类器，用于从另一个骨干模块提取的语义表示。我们使用BERT (Devlin et al.， 2019)，因为它具有强大的语义能力。将图表示与BERT相结合，SpellGCN可以利用相似性知识，并相应地生成正确的修正。对于表1中的例子，SpellGCN能够在发音约束下正确地修改句子。
实验在三个开放基准上进行。结果表明，SpellGCN提高BERT明显，优于所有竞争的模型，在很大程度上。
![[attachments/Pasted image 20220713202415.png]]
总而言之，我们的贡献如下:
		- 我们提出了一种新颖的端到端可训练的SpellGCN，以整合语音和形状相似性到语义空间。它的重要组成部分，如专门的图卷积和注意的组合操作被仔细研究。
		- 我们从定量和定性两方面研究了SpellGCN的性能。实验结果表明，该方法在三个基准数据集上均取得了最佳的测试结果。
### 2. Related Work
CSC任务是一个长期存在的问题，引起了社会各界的广泛关注。这方面的研究是近年来出现的(Jia等，2013;Xin等，2014;于、李，2014;Tseng等人，2015;Fung等人，2017;Wang et al.， 2019;Hong等人，2019)，以及其他主题，例如，语法错误纠正(GEC) (Rao等人，2018;Ji等人，2017;Chollampatt等人，2016;葛等人，2018)。CSC侧重于检测和纠正字符错误，而GEC也包括需要删除和插入的错误。以前的工作使用非监督语言模型处理CSC (Liu等人，2013;Yu和Li, 2014)。通过评价句子/短语的困惑度来检测/纠正错误。
然而，这些模型无法根据输入的句子进行校正。为了解决这个问题，我们对CSC采用了几种区别性序列标记方法(Wang et al.， 2018)。为了获得更大的灵活性和更好的性能，还采用了几种序列对序列的模型(Wang et al.， 2019;Ji等人，2017;Chollampatt等人，2016;Ge等人，2018)，以及BERT (Hong等人，2019)。
近年来，利用汉字相似性的外部知识引起了人们的重视。相似性知识可以收集到一个字典，即混淆集，其中存储相似对。Yu和Li(2014)首先使用字典来检索潜在错误的相似候选。Wang等人(2019)将复制机制纳入循环神经模型。当给定相似的字符作为输入时，它们的模型使用复制机制直接将字符复制到目标句子中。从某种意义上说，由于相似性信息仅用于候选者的选择，这些模型很难对相似字符之间的关系进行建模。为了捕获语音/形状相似性并探索字符之间的先验相关性，我们提出使用图卷积网络(GCN) (Kipf和Welling, 2017)来建模字符之间的相关性，并结合BERT的预训练(Devlin等人，2019;Cheng et al.， 2019)来解决CSC任务。
GCN已被应用于几个任务上的关系建模。Yan等人(2019)将其应用到关系提取任务中，关系构建层次树。Li等人(2018);Cheng等人(2018)利用它建立时空模型来预测交通流。GCN还用于在多标签任务中对标签之间的关系建模(Chen等人，2019年)。在本文中，GCN首次成功地应用于CSC任务中。CSC中的关系与那些图中对象在语义上相关的任务有很大的不同。相比之下，相似的字符在语义上却截然不同。因此，我们深入研究了我们的SpellGCN的影响，并提出了几个基本的技术。
### 3.  Approach
在这一节中，我们详细阐述了CSC的方法。首先，给出了问题的表达式。然后，我们介绍了SpellGCN的动机，然后详细描述。最后，给出了其在CSC任务中的应用。
#### 3.1 Problem Formulation
中文拼写检查任务的目的是检测和纠正中文中的错误。当给定一个文本序列$X = \{x_1, x_2，…， x_n\}$，由n个字符组成，模型以X为输入，输出一个目标字符序列$Y = \{y_1, y_2，…, y_n\}$。我们通过对条件概率p(Y|X)进行建模并使其最大化，将该任务表述为一个条件生成问题。
#### 3.2 Motivations
所提方法的框架如图1所示。它由两个组件组成，即字符表示提取器和SpellGCN。提取器为每个字符派生一个表示向量。在提取器上面，使用SpellGCN对字符之间的相互依赖进行建模。交互后输出包含相似字符信息的目标向量。
![[attachments/Pasted image 20220713203600.png]]
如表1所示，普通语言模型能够提供可行的语义修正，但在满足发音约束方面存在困难。虽然修正“月消费最”在语义上是可信的，它的语音与“换经费产”和“环境非常”有很大不同。这说明字符之间的相似度信息是模型学习生成相关答案的必要条件。以前的方法已经考虑到这种相似性。然而，他们通常把相似的字符作为潜在的候选者，忽略了它们在发音和形状方面的相互关系。本文对这一问题进行了初步的尝试，试图将符号空间(语音和视觉相似性知识)和语义空间(语言语义知识)融合到一个模型中。为此，我们利用图神经网络(GNN)的强大功能，直接注入相似性知识。其基本思想是通过聚集相似字符之间的信息来更新表示。直观地说，当使用我们的方法时，模型可能会有类似符号的感觉。
在各种GNN模型中，我们在实现中使用GCN。由于图中有多达5K的汉字，所以轻量级的GCN更适合我们的问题。SpellGCN详细描述如下。
#### 3.3 Structure of SpellGCN
SpellGCN需要两个相似图$A^p$, $A^s$对应的发音和形状相似，这是从一个开源的混淆集(Wu et al.， 2013)。为简单起见，如果没有必要，上标将被省略，并且A表示这两个相似图中的一个。每个相似图是一个大小为$R^{N×N}$的二值邻接矩阵，由混淆集中的N个字符构造而成。第i个字符与第j个字符之间的边$A_{i,j}∈{0,1}$表示混淆集中是否存在(i, j)对。
SpellGCN的目标是学习一个映射函数，通过a定义的卷积运算，将第l层的输入节点嵌入$H^l∈R^{N×D}$(其中D是字符嵌入的维数)映射到一个新的表示$H^{l+1}$。这个映射函数有两个主要的子组件:图卷积运算和注意力图组合运算。
**Graph Convolution Operation**
图卷积运算是从图中相邻字符中吸收信息。每一层采用GCN (Kipf and Welling, 2017)中的轻量级卷积层:
$$f (A, H^l) = \hat AH^lW^l_g ,$$
其中$W^l_g \in R^{D\times D}$是一个可训练的矩阵，而$\hat A \in R^{N \times N}$是相邻矩阵A的归一化版本。对于$\hat A$的定义，我们建议你阅读原始论文(Kipf和Welling, 2017)。注意，我们使用BERT的字符嵌入作为初始节点特征$H^0$，并省略了卷积后的非线性函数。由于我们采用BERT作为提取器，它有自己的学习语义空间，所以我们从方程中去掉激活函数，以保持导出的表示与原始空间相同，而不是完全不同的空间。在我们的实验中，使用像ReLU这样的非线性激活是无效的，导致性能下降。
**Attentive Graph Combination Operation**
图卷积运算处理单个图的相似性。为了将发音相似图和形状相似图结合起来，采用了注意机制(Bahdanau et al.， 2015)。对于每个字符，我们表示如下的组合操作:
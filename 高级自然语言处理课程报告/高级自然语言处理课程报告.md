## 中文文本校对（Chinese Spell Check, CSC)
刘鑫
202121198408
计算机科学与技术

#### 1. 问题描述
中文拼写检查（CSC）任务的目的是检测并纠正中文文本中的拼写错误（spelling errors）。如下图所示，中文中许多汉字语音（phonologically）或形态（visually）上相近，但在语义上有明显差异，极易在拼写过程中输入错误。
![[Pasted image 20220708192837.png]] 

在自然语言处理任务中，可以把中文拼写检查任务形式化定义为给定一个包含$n$个字符的序列$X=\{x_1, x_2,...,x_n\}$，$X$为模型的输入，输出$Y=\{y_1,y_2,...,y_n\}$为修改序列，输入序列与修改序列等长。可以把该任务视为一种受限的生成或序列标注任务，目标是最大化条件概率$p(Y|X)$。

#### 2. 前人工作
##### Realise
[Realise](https://arxiv.org/abs/2105.12306)是腾讯实验室2021年发表在ACL的工作，作者基于以下三个假设提出了Realise模型：
- 判断字形和字音的混淆关系与其字音或字形的顺序无关，而与其构成成分强相关。
- 两个字之间的混淆关系有可能同时决定于字音和字形。
- 相对于时序/卷积模型，基于Self-attention的架构能够更好的捕捉上述关系。

模型结构如下图所示：
>![[Pasted image 20220708192944.png]]

Realise模型由三个编码器构成，分别是语义编码器、字音编码器和字形编码器，三个编码器分别编码输入文本的语义、字音和字形信息，编码后的信息经过门控机制后输入到Transformer编码器进一步编码，最后经过输出层获得修改后的序列。
该模型充分利用了字音和字形的信息，在拼写检查任务上有了显著提升，论文给出实验结果如下图所示。
![[Pasted image 20220708193031.png]]

<div STYLE="page-break-after: always;"></div>

##### ELACTRA
BERT 在language model中取得了很多非常好的成就，但是BERT的MLM的实现，并不是非常高效的，只有15%的tokens对参数的更新有用，其他的85%是不参与gradients的update的，并且存在了预训练和fine-tuning的mismatch，因为在fine-tuning阶段，并不会有\[MASK\]的token。基于对上述训练问题，[ELECTRA](https://arxiv.org/abs/2003.10555)提出了一种新的预训练方式——Replaced token detection。
作者做了如下改进：
 
 - 提出了新的模型预训练的框架，采用generator和discriminator的结合方式，但又不同于GAN
 - 将Masked Language Model的方式改为了replaced token detection
 - 因为masked language model 能有效地学习到context的信息，所以能很好地学习embedding，所以使用了weight sharing的方式将generator的embedding的信息共享给discriminator
 - discriminator 预测了generator输出的每个token是不是original的，从而高效地更新transformer的各个参数，使得模型的熟练速度加快
 - 该模型采用了小的generator以及discriminator的方式共同训练，并且采用了两者loss相加，使得discriminator的学习难度逐渐地提升，学习到更难的token（plausible tokens）
 - 模型在fine-tuning 的时候，丢弃generator，只使用discriminator
  
  模型架构如下图所示：
  ![[Pasted image 20220708193045.png]]
  
  
  #### 3. 研究方法与内容
  CSC任务本质上是将错误拼写的token修改回原本正确的token，Realise模型架构中的Semantic Encoder使用的是Bert，而Bert与CSC任务存在一定Pre-training和Fine-turning间的Mismatch。
  ELECTRA的Replace token detection的预训练方式更契合CSC的任务模式，因此想尝试用ELECTRA的预训练方式训练一个Semantic Encoder，以此缩小Pre-training和Fine-turning的Mismatch，探究这种方式能否对模型效果有所提升。
  
  出于学习的目的，需要将Realise自己复现一遍，然后再探究ELECTA的应用。
  
  #### 4. 进展
  
  由于初次动手从零开始复现工作，一些代码细节不太熟悉，进度较慢，到目前为止，刚写完三个编码器中的字音编码器的预训练部分，但该部分的模型结构也还没有调通，下面主要记录本次课程作业中的收获。

  
##### DataLoader的使用

  Realise代码中对数据集的加载采用手动构建数据集，手动构建batch的方式，我使用pytorch自带的DataLoader类进行数据集的加载和构建batch。第一次动手完成了数据集的tokenization, padding以及build batch。
  DataLoader的工作流程如下图：
  ![[Pasted image 20220708193055.png]]
  DataLoader通过Sampler决定读取哪些数据，也就是从Dataset的哪里读取，读取到数据后，需要一个collate_fn 函数对读取的数据进行整理，这个函数中主要是进行数据的padding操作，经过整理后的数据即为一个batch。
  
##### deepspeed的使用

  deepspeed是微软发布的多机多卡分布式训练python库，我根据师兄的代码，将其简单应用在了realise的训练上。
  
  deepspeed的初级使用比较简单，初始化后就能自动实现模型的分布式训练，初始化代码如下：
  
```python
self.model, self.optimizer, _, self.scheduler = deepspeed.initialize(
								model=model,
		                        model_parameters=model.parameters(),
		                        config=config)
```
  
  需要注意的是，deepspeed的参数设置较为复杂，都写在cofig.json中：
```
  {
    "train_batch_size": 64,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "AdamW",
        "params": {
        "lr": 1e-5
        }
    },
    "gradient_clipping": 1.0,
    "fp16": {
        "enabled": false
    },
    "amp": {
        "enabled": true,
        "opt_level": "O1",
        "loss_scale": "dynamic"
    }
}
```
##### Phonetic Encoder的实现
模型代码如下：
```python
class PhoPretrain(nn.Module):
    """The Phonetic Encoder"""
    def __init__(self, vocab_size, pho_vocab_size, hidden_size, num_encoder_layers=4, num_encoder_heads=12):
        super().__init__()
        self.vocab_size = vocab_size
        self.pho_embeddings = nn.Embedding(pho_vocab_size, hidden_size, padding_idx=0)
        # The Character-level Encoder, uni-directional GRU 
        self.pho_gru = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False,
        )
        # The Sentence-level Encoder, a 4-layer Transformer with the same hidden size as the semantic encoder.
        self.pho_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(hidden_size, 
                                    num_encoder_heads, 
                                    batch_first=True),
            num_layers=num_encoder_layers,
            norm=nn.LayerNorm(normalized_shape=hidden_size)
        )

        self.cls = PhoOutputCls(vocab_size, hidden_size)

    def forward(self, batch):
        input_ids = batch['tgt_idx']
        attention_mask = batch['masks']
        loss_mask = batch['loss_mask']
        pho_idx = batch['pho_idx']
        pho_lens = batch['pho_lens']

        input_shape = input_ids.size()

        pho_embeddings = self.pho_embeddings(pho_idx)
        pho_embeddings = torch.nn.utils.rnn.pack_padded_sequence(
            input=pho_embeddings,
            lengths=pho_lens.cpu(),
            batch_first=True,
            enforce_sorted=False,
        )

        _, pho_hiddens = self.pho_gru(pho_embeddings)
        import pdb;pdb.set_trace()
        pho_hiddens = pho_hiddens.squeeze(0).reshape(input_shape[0], input_shape[1], -1).contiguous()
        sequence_output = self.pho_model(src=pho_hiddens, src_key_padding_mask=attention_mask)

        prediction_scores = self.cls(sequence_output)

        loss_fn = CrossEntropyLoss()
        # Only keep active parts of the loss
        active_loss = loss_mask.view(-1) == 1
        active_logits = prediction_scores.view(-1, self.vocab_size)[active_loss]
        pred_labels = active_logits.argmax(dim=-1)
        gold_labels = input_ids.view(-1)[active_loss]
        loss = loss_fn(active_logits, gold_labels)
        return (loss, pred_labels, gold_labels, )
```
Phonetic Encoder由一个字符级别的编码器和一个句子级别的编码器组成， 字符级别编码器由单层GRU构成，编码字符级别拼音信息，句子级别的编码器由4个hidden_size为768维的TransformerEncoder构成，编码句子级别拼音信息。

##### 目前的问题
主预训练流程已经写完了，但是模型的数据流动过程还有点问题，没有完全跑通。
![[Pasted image 20220708193118.png]]

#### 总结与展望
初次动手写大型项目，进度很慢，同时研究计划的对ELECTA的了解也还没有开始，在接下来的时间里面希望能够继续完成还没完成的计划。
但进度虽慢，收获也是蛮多，从前人的代码中学习到了很多，包括对数据的处理和整个模型的训练流程，虽然还没有走通，但一遍写下来，体会也比较深刻。
希望在接下来的日子里能够顺利完成没有完成的研究计划，在失败中进步，能够做出东西来。
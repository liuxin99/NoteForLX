#### 数据预处理

1、通常使用float32作为数据类型，float64运算比较慢

2、数据缺失时，可以根据数据情况做插值

#### 线性代数

计算F范数 torch.norm() 将矩阵拉开为向量

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1636427707449-1450e7f0-851f-46e4-8c43-c44d60981e2f.png)

#### 矩阵计算

1、标量对向量求导，向量对标量求导，结果为向量，向量对向量求导，结果为矩阵

2、梯度方向指向函数的值变化最大的方向

#### 自动求导

神经网络耗内存资源的一点在于需要储存正向求导过程中的所有中间结果

深度学习中的loss一般是个标量

同等数据下，batchsize越小，对收敛越好

#### Softmax

![](https://cdn.nlark.com/yuque/__latex/db7502fe1bb17374e627a729d40c585b.svg)

![](https://cdn.nlark.com/yuque/__latex/83b797b0ade3ac0993fe6a6c20cc7a23.svg)

概率y和![](https://cdn.nlark.com/yuque/__latex/13ada0b94f610ad731b13dd5262af022.svg)的区别作为损失

交叉熵常用来衡量两个概率的区别

![](https://cdn.nlark.com/yuque/__latex/2d4d2d01c8250c72374521550e6edd02.svg)

将它作为损失： ![](https://cdn.nlark.com/yuque/__latex/8371fa0116b20d8c3dc81976db2dc802.svg) （![](https://cdn.nlark.com/yuque/__latex/8d62e469fb30ed435a668eb5c035b1f6.svg)中只有一个为1，其他为0)

其梯度是真实概率 与预测概率的区别： ![](https://cdn.nlark.com/yuque/__latex/71c10723fe69e4481895af3ebf0df293.svg)

softmax回归是一个多分类模型，使用Softmax操作子可以得到每个类的预测置信度，使用交叉熵来衡量预测和标号的区别

#### 常用简单损失函数

**L2 Loss（均方误差）：** ![](https://cdn.nlark.com/yuque/__latex/9999dee88e5806d666ab4ef3b764f779.svg)

蓝色为loss，绿色为似然函数，黄色为loss梯度（导数）

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1637461658665-07d4578b-c16a-48e5-836a-39789b434cfa.png)

**L1 Loss （绝对值损失）：**![](https://cdn.nlark.com/yuque/__latex/03a6f969518fe3aee201e649de2e21dc.svg)

梯度为常数，比较稳定，但在零点处不可导，优化到后期会不稳定，发生剧烈震荡

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1637461625525-fb9798fb-a71e-493c-bc59-80ff2d2be945.png)

**Huber's Robust Loss：**

![](https://cdn.nlark.com/yuque/__latex/a135c878422418ddfdd4d05f61d91862.svg)

当预测值和真实值差得远时，比较平滑，比较近时是渐变的

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1637462360552-f8384b5d-d168-4542-8b8a-c346884bf413.png)

#### 常用激活函数

**Sigmoid 激活函数** ： 将输入投影到（0, 1）范围

![](https://cdn.nlark.com/yuque/__latex/34ad5c71dbafe72808aaf45097dad10c.svg)

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1639405557543-df9b993c-1f7f-4a3b-a97b-26c4d327a13b.png)

Tanh 激活函数： 将输入投影到（-1,1）范围

![](https://cdn.nlark.com/yuque/__latex/5750662f3962393961b253eaf3adcb0b.svg)

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1639405734603-ccfacea2-5170-43f1-a068-3f6e2c07f698.png)

**ReLU（Rectified linear unit） 激活函数**

![](https://cdn.nlark.com/yuque/__latex/246be30548366c57c4a064104795ac9e.svg)

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1639405934856-d984b9b9-6106-43a2-9710-f38a36aa6996.png)

#### 模型选择 + 过拟合和欠拟合

**训练误差和泛化误差**

-   训练误差：模型在训练数据上的误差
-   泛化误差：模型在新数据上的误差

-   验证数据集：一个用来评估模型好坏的数据集，不能跟训练数据混在一起（用来调整超参数）
-   测试数据集：只用一次的数据集

-   **K-折交叉验证：**在没有足够多数据时使用

	-   算法

		-   将训练数据分割为K块
		-   for i = 1, ... , K

			-   使用第i块作为验证集，其余作为训练数据集

		-   报告K个验证集误差的平均

	-   常用：K=5或10

**过拟合和欠拟合**

![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1639470402451-0f2956ef-74ab-4564-a106-99b1861cfb63.png)

-   模型容量

	-   拟合各种函数的能力
	-   低容量的模型难以拟合训练数据

	-   高容量的模型可以记住所有训练数据
-   ![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1639471058161-76cbca0d-3f92-4f05-ae8f-708bf8306cb5.png)

-   估计模型容量

	-   难以在不同的种类算法之间比较

		-   例如树模型和神经网络

	-   给定一个模型种类，将有两个主要因素

		-   参数的个数
		-   参数值的选择范围

##### 权重衰退

**使用均方范数作为硬性限制**

-   通过限制参数值的选择范围来控制模型容量（**w**可选的范围越大，模型越复杂）

	-   ![](https://cdn.nlark.com/yuque/__latex/e6655cf383b161745aa9955072ba88c0.svg)
	-   通常不限制偏移 b

	-   小的![](https://cdn.nlark.com/yuque/__latex/2554a2bb846cffd697389e5dc8912759.svg)意味着更强的正则项

**使用均方范数作为柔性限制**

-   对每个![](https://cdn.nlark.com/yuque/__latex/2554a2bb846cffd697389e5dc8912759.svg)，都可以找到![](https://cdn.nlark.com/yuque/__latex/c6a6eb61fd9c6c913da73b3642ca147d.svg) 使得之前的目标函数等价于下面

	-   ![](https://cdn.nlark.com/yuque/__latex/5480c4357e8c393280652c932cbce7bc.svg)
	-   可以通过拉格朗日乘子来证明

-   超参数![](https://cdn.nlark.com/yuque/__latex/c6a6eb61fd9c6c913da73b3642ca147d.svg) 控制了正则项的重要程度

	-   ![](https://cdn.nlark.com/yuque/__latex/726b3f22ce0493a4e51e06141e079545.svg) ： 无作用
	-   ![](https://cdn.nlark.com/yuque/__latex/af82f5bc7ae1e61bb59909f11e994548.svg)

-   参数更新法则

	-   ![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1640096773849-15f3d8ea-6f6b-46ec-836c-6841249094a3.png)

-   权重衰退通过L2正则项使得模型参数不会过 大，从而控制模型复杂度
-   正则项权重是控制模型复杂度的超参数 pytorch 中 超参为SGD里面的 **weight_decay** 一般取 e-3, e-2

**丢弃法**

-   动机

	-   一个号的模型需要对输入数据的扰动鲁棒

	-   使用有噪音的数据等价于Tikhonov正则
	-   丢弃法：在层之间加入噪音

-   无偏差的加入噪音

	-   对**x**加入噪音得到![](https://cdn.nlark.com/yuque/__latex/30b94bbbad526eeb6dd345afdaeaccf8.svg)，我们希望![](https://cdn.nlark.com/yuque/__latex/fcc490036f307c2be31c65c34daa9471.svg)
	-   丢弃法对每个元素进行如下扰动

	-   ![](https://cdn.nlark.com/yuque/__latex/17f1887cd95132598a93a04a2f4e4871.svg)
	-   ![](https://cdn.nlark.com/yuque/__latex/85f68eaf426ad73d86ab8712aac386dc.svg)

-   使用丢弃法

	-   通常将丢弃法作用在隐藏全连接层的输出上
	-   ![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1640269740662-3a0e525b-ff29-4d54-95c9-20ed7002e066.png)

-   推理中的丢弃法

	-   正则项只在训练中使用：他们影响模型参数的更新
	-   在推理过程中，丢弃法直接返回输入

	-   ![](https://cdn.nlark.com/yuque/__latex/591005d8b0abdcbb2ecec55f8c2bc8d3.svg)
	-   这样也能保证确定性的输出

-   总结

	-   丢弃法将一些输出项随机置0来控制模型复杂度
	-   常作用在多层感知机的隐藏层输出上

	-   丢弃概率是控制模型复杂度的超参数

#### 数值稳定性

**神经网络的梯度**

-   考虑如下有d层的神经网络（t为层数）

![](https://cdn.nlark.com/yuque/__latex/3375bae8a0664ed562e8c3157d69c853.svg)

-   计算![](https://cdn.nlark.com/yuque/__latex/2db95e8e1a9267b7a1188556b2013b33.svg) 关于参数![](https://cdn.nlark.com/yuque/__latex/75565270da0c2b034ed41c4b4bf3c3ee.svg)的梯度

![](https://cdn.nlark.com/yuque/__latex/fd7fcf8d0fb729e54eeeef0625e45c76.svg) （d-t 次矩阵乘法）

数值稳定性的常见两个问题：

-   梯度爆炸
-   梯度消失

**梯度爆炸的问题**

-   值超出值域（infinite）

	-   对于16位浮点数尤为严重（数值区间6e-5 - 6e4）

-   对学习率敏感

	-   如果学习率太大-> 大参数值 -> 更大的梯度
	-   如果学习率太小 -> 训练无进展

	-   我们可能需要在学习过程中不断调整学习率

**梯度消失的问题**

-   梯度值变成0

	-   对16位浮点数尤为严重

-   训练没有进展

	-   不管如何选择学习率

-   对底部层尤为严重

	-   仅仅顶部层训练的较好
	-   无法让神经网络更深

当数值过大或者过小时会导致数值问题

常常发生在深度模型中，因为其会对n个数累乘

**让训练更加稳定**

-   目标：让梯度值在合理的范围内

	-   例如[1e-6, 1e3]

-   将乘法变加法

	-   ResNet, LSTM

-   归一化

	-   梯度归一化， 梯度裁剪（强行将梯度限制在一定范围内）

-   合理的权重初始和激活函数

	-   让每层的方差是一个常数

		-   将每层的输出和梯度都看作随机变量
		-   让它们的均值和方差都保持一致

-   权重初始化

	-   在合理值区间里随机初始化参数
	-   训练开始的时候更容易有数值不稳定

		-   远离最优解的地方损失函数表面可能很复杂
		-   最优解附近表面会比较平滑

	-   使用![](https://cdn.nlark.com/yuque/__latex/25418dfae19d3ebd269ba4b6418ae8db.svg)来初始可能对小网络没问题，但不能保证深度神经网络
	-   ![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1640517763016-8e6873f1-755f-4050-a2ac-ffa4a2251ea2.png)

	-   ![](https://cdn.nlark.com/yuque/0/2021/png/22995599/1640518051342-a5e593e7-8dd9-4e5c-ad0d-80c04dc0a131.png)
	-   总结：合理的权重初始值和激活函数的选取可以提升数值稳定性

#### pytorch神经网络基础

**自定义块**

-   任何模型都应该是nn.Module的子类

**参数访问**

-   Squiential() 可以看做一个模型的list，支持下标访问
-   net.state_dict() 查看模型参数

-   net.named_parameters() 查看模型所有参数

**初始化方法**

-   **nn.init.xavier_uniform()**
-   **nn.init.constant_()**

**模型的加载和保存**

-   **torch.save(object, filename)**
-   **torch.load(object, filename)**

-   保存模型参数

	-   **torch.save(net.state_dict(), "net.params")**
	-   **net.load_state_dict(torch.load("net.params"))**

**计算设备**

-   **torch.device("cpu")**
-   **torch.cuda.device("cuda")**

-   **torch.cuda.device("cuda: 1")**
-   **查询可用设备：torch.cuda.device_count()**

-   **查看张量所在设备：tensor.device()**
-   **存到GPU：X = torch.tensor(shape, device=try_gpu())**

-   **net.to(device)**

##### 序列模型

-   对条件概率建模：![](https://cdn.nlark.com/yuque/__latex/ad0b860bd417bc7c325708898755e073.svg)
-   对见过的数据建模，也称自回归模型

**马尔科夫假设**

-   假设当前数据只跟过去![](https://cdn.nlark.com/yuque/__latex/a6f317b268ae825d94f832f970af607c.svg)个数据点相关：![](https://cdn.nlark.com/yuque/__latex/a88b1f58d6e1b268aaffc745c915ef24.svg)

**潜变量模型**

-   引入潜变量![](https://cdn.nlark.com/yuque/__latex/6c4ff69dbcc329835a33b80fe3a145c7.svg)来表示过去信息![](https://cdn.nlark.com/yuque/__latex/102491456d3dbdc352d7a9677c1732d5.svg)
-   这样![](https://cdn.nlark.com/yuque/__latex/166b767892833b8594482401102d77ac.svg)

-   模型被划分为两块：

	-   怎样根据过去的数据更新潜变量
	-   怎样根据过去的数据和现在的潜变量更新当前状态

**总结**

-   时序模型中，当前数据跟之前观察到的数据相关
-   自回归模型使用自身过去数据来预测未来

-   马尔科夫模型假设当前只跟最近少数数据相关，从而简化模型
-   潜变量模型使用潜变量来概括历史信息

#### 语言模型

-   给定文本序列![](https://cdn.nlark.com/yuque/__latex/3aa4e7cac6382738e7bbabdcaa759034.svg)， 语言模型的目标是轨迹联合概率![](https://cdn.nlark.com/yuque/__latex/503c2bdd88f661e6b0197b8fa48043ce.svg)
-   它的应用包括：

	-   做预训练模型
	-   生成文本，给定前面几个词，不断的使用![](https://cdn.nlark.com/yuque/__latex/52b352568e19c8b79a490999f442c647.svg)来生成后续文本

	-   判断多个序列中哪个更常见

-   使用统计方法时常使用N元语法

#### 循环神经网络RNN

![](https://cdn.nlark.com/yuque/0/2022/png/22995599/1642773996097-35cc9cae-0a03-438b-afa9-fe812c23f03e.png)

-   更新隐藏状态： ![](https://cdn.nlark.com/yuque/__latex/73c1ca258d2ed55a0fb7a8fcafc974fc.svg) （去掉第一项就退化为MLP）
-   输出： ![](https://cdn.nlark.com/yuque/__latex/a5d4d8cad909fc3776735bbb8f9a74f8.svg)

**困惑度（perplexity）**

-   **衡量一个语言模型的好坏可以用平均交叉熵**

![](https://cdn.nlark.com/yuque/__latex/e60d701f0324ff4bfed15aea8788a487.svg)

p是语言模型的预测概率， ![](https://cdn.nlark.com/yuque/__latex/cf7ee950cf61a6003c0ec4af7971d8a8.svg)是真实词

-   历史原因NLP使用困惑度![](https://cdn.nlark.com/yuque/__latex/caf119b3b5817a239fc67609da188194.svg)来衡量，平均每次可能的选项

	-   1表示完美， 无穷大是最差情况

**梯度裁剪**

-   迭代中计算这T个时间歩上的梯f度，在反向传播过程中产生长度为![](https://cdn.nlark.com/yuque/__latex/43995c439a3df1ae219e6814777e8ec7.svg)的矩阵乘法链，导致数值不稳定
-   梯度裁剪能有效预防梯度爆炸

	-   如果梯度长度超过![](https://cdn.nlark.com/yuque/__latex/2554a2bb846cffd697389e5dc8912759.svg)， 那么拖影回长度 ![](https://cdn.nlark.com/yuque/__latex/2554a2bb846cffd697389e5dc8912759.svg)

![](https://cdn.nlark.com/yuque/__latex/0f6f249b5984922b47923c25bc4927e8.svg)

![](https://cdn.nlark.com/yuque/0/2022/png/22995599/1642775905151-df4f832c-9991-4964-8e27-a58386a040f1.png)

**总结**

-   循环神经网络的输出取决于当下输入和前一个时间的隐变量
-   应用到语言模型中时， 循坏神经网络根据当前词预测下一时刻词

-   通常使用困惑度来衡量语言模型的好坏